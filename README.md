<p align="center">
  <h1 align="center">DiET-GS ðŸ«¨ <br>
Diffusion Prior and Event Stream-Assisted <br>
Motion Deblurring 3D Gaussian Splatting</h1>
  <p align="center">
    <a href="https://www.linkedin.com/in/seungjun-lee-43101a261/">Seungjun Lee</a></span> Â·  
    <a href="https://www.comp.nus.edu.sg/~leegh/">Gim Hee Lee</a><sup></sup> <br>
    Department of Computer Science, National University of Singapore<br>
  </p>
  <h2 align="center">CVPR 2025</h2>
  <h3 align="center"><a href="https://github.com/DiET-GS/DiET-GS">Code</a> | <a href="">Paper</a> | <a href="https://diet-gs.github.io">Project Page</a> </h3>
  <div align="center">
  <a href="https://pytorch.org/get-started/locally/"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-ee4c2c?logo=pytorch&logoColor=white"></a>
  </div>
</p>

<p align="center">
  <a href="">
    <img src="https://github.com/DiET-GS/DiET-GS/blob/main/teaser.png" alt="Logo" width="100%">
  </a>
</p>
<p align="center">
Our <strong>DiET-GS++</strong> enables high quality novel-view synthesis with recovering precise color and well-defined details from the blurry multi-view images.
</p>
</p>

## News:

- [2025/02/27] DiET-GS is accepted to CVPR 2025 ðŸ”¥. The code will be released at early June.

## TODO
- [ ] Release the code

## Citation
If you find our code or paper useful, please cite
```bibtex
@article{lee2024segment,
      title = {Segment Any 3D Object with Language}, 
      author = {Lee, Seungjun and Zhao, Yuyang and Lee, Gim Hee},
      year = {2024},
      journal   = {arXiv preprint arXiv:2404.02157},
}
```
